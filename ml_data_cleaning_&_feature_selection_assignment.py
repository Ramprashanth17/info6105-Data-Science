# -*- coding: utf-8 -*-
"""ML Data Cleaning & Feature Selection Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15MhJ6JH2BnQPj3tRzgHyPnx-qjxfyI-B

# Abstract

Prediction of levels of obesity by using machine learning classification models.

Data collected from UCI Machine Learning Repository.

# About the Data

Dietary, exercise and personal daily habits of individuals from Mexico, Peru and Columbia are recorded to build estimation of obesity levels.

Obesity Level will be used as the target (y) variable, which consists of 7 classes - Insufficient Weight, Normal Weight, Overweight Level I, Overweight Level II, Obesity Type I, Obesity Type II and Obesity Type III.

There are 17 attributes in total related to individual habits that are likely to determine obesity levels, such as number of main meals, time using technology devices, gender and transportation used.

Details of the questions and possible answers collected for the data can be found in the link provided above.
"""

# installing dependencies
!pip install eli5

# Importing the required packages
import pandas as pd
import numpy as np
import seaborn as sn
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, FunctionTransformer, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression


from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE


from sklearn.pipeline import Pipeline
# import category_encoders as ce

import warnings
warnings.filterwarnings("ignore")

pd.set_option('display.max_columns', None)

"""# Exploratory Data Analysis

"""

df = pd.read_csv('/content/ObesityDataSet_raw_and_data_sinthetic.csv')
df.head()

# Checking for any missing values
null_values = df.isnull().sum()

# Checking for duplicate values
duplicate_values = df.duplicated().sum()

print(f"Number of null values in the dataframe is: \n {null_values}")
print(f"Number of duplicate values in the dataframe is: {duplicate_values}")

"""# Are there missing values?

No, the data doesn't have any missing values
"""

# Dropping the duplicated values
df = df.drop_duplicates()

df.duplicated().sum()

"""# What are the data types?

The dataset contains both numerical and categorical data types as shown below.
"""

columns = df.columns
categorical_columns = [c for c in columns if df[c].dtypes == 'object']
categorical_columns

for i in categorical_columns:
    print(f"{i} : {df[i].unique()}")

numerical_columns = [col for col in df.columns if df[col].dtype in ['int64', 'float64']]
print(numerical_columns)

df.describe()

#Plotting the distributions to visualize the numerical variables
for col in df.select_dtypes(include=['int64', 'float64']):
  plt.hist(df[col])
  plt.title(f"Distribution of {col}")
  plt.xlabel(col)
  plt.ylabel("Frequency")
  plt.show()

"""# What are the likely distributions of the numeric variables?

As shown in the graphs, some numerical variables follow normal distribution while some follow binomial distribution.

"""

# Likely distributions of the numerical columns
# Box plots
df.boxplot(column=['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE'])

# checking the distribution of independent variables
from statsmodels.graphics.gofplots import qqplot

for col in df.select_dtypes(include=['int64', 'float64']):
    plt.figure(figsize=(8, 5))
    fig = qqplot(df[col], line="45", fit="True")
    plt.xticks(fontsize=13)
    plt.yticks(fontsize=13)
    plt.xlabel("Theoretical quantiles", fontsize=15)
    plt.ylabel("Sample quantiles", fontsize=15)
    plt.title("Q-Q plot of {}".format(col), fontsize=16)
    plt.grid(True)
    plt.show()

df1 = df.copy()

df1

# Finding independent variables used to predict target value

# Method-1: Using Co-relation Analysis:
# Calculate correlations
df1.corr()

"""# Finding independent variables used to predict target value

Used Co-relation, Domain Knowledge and Random Feature Elimination
"""

plt.figure(figsize=(15,10))
sn.heatmap(df.corr(), annot = True)

"""# Which independent variables have missing data? How much?

No, the independent variables doesn't have missing data.
"""

X = df1.iloc[:,:-1]
y = df1.iloc[:, -1]
y = pd.DataFrame(y, columns=['NObeyesdad'])

labels = ['Normal_Weight', 'Overweight_Level_I', 'Overweight_Level_II', 'Obesity_Type_I', 'Insufficient_Weight', 'Obesity_Type_II', 'Obesity_Type_III']

# Create a dictionary to map labels to numbers starting from 1
label_to_number = {label: i + 1 for i, label in enumerate(labels)}

# Map the condition labels to numeric values
y['NObeyesdad'] = y['NObeyesdad'].map(label_to_number)
y.head()

# Custom function for binary encoding
def binary_encode(df1, columns):
    obesity = df1.copy() #ensure that the function is safe and does not modify the DataFrame passed to it unexpectedly.
    label_encoder = LabelEncoder()
    for col in columns:
        obesity[col] = label_encoder.fit_transform(obesity[col])
    return obesity

!pip install category_encoders

import category_encoders as ce
# Columns for binary encoding, one-hot encoding, and category encoding
binary_columns = ['Gender', 'FAVC', 'SMOKE', 'SCC', 'family_history_with_overweight']
onehot_encoded_columns = ['CALC', 'MTRANS']
category_encoded_columns = ['CAEC']

# Creating a pipeline for binary encoding
binary_pipeline = Pipeline(steps=[
    ('binary_encode', FunctionTransformer(binary_encode, kw_args={'columns': binary_columns}))
])

# Creating a pipeline for category encoding
category_pipeline = Pipeline(steps=[
    ('category_encode', ce.OrdinalEncoder(cols=category_encoded_columns))
])

# Creating the ColumnTransformer
categorical_transformer = ColumnTransformer(
    transformers=[
        ('binary', binary_pipeline, binary_columns),
        ('onehot', OneHotEncoder(), onehot_encoded_columns),
        ('category', category_pipeline, category_encoded_columns)
    ],
    remainder='passthrough'  # leave the rest of the columns unchanged
)

# Applying the ColumnTransformer to the DataFrame
X_categorical_transformed = categorical_transformer.fit_transform(X)

# Getting new column names
binary_encoded_names = binary_columns  # Binary encoded columns names remain the same
onehot_encoded_names = categorical_transformer.named_transformers_['onehot'].get_feature_names_out(onehot_encoded_columns)
category_encoded_names = categorical_transformer.named_transformers_['category'].get_feature_names_out(category_encoded_columns)

new_columns = binary_encoded_names + list(onehot_encoded_names) + list(category_encoded_names)
new_columns += [col for col in X.columns if col not in binary_columns + onehot_encoded_columns + category_encoded_columns]

# Creating the new DataFrame with the new column names
obesity_encoded_obesity = pd.DataFrame(X_categorical_transformed, columns=new_columns)

obesity_encoded_obesity.head()

"""# Do the training and test sets have the same data?

No, the training and test sets are different. The data has been splitted.

# Is the predictor variables independent of all the other predictor variables?

Yes, except for Weight all the other predictor variables are independent.

# Which predictor variables are the most important?

Weight, Height, FACVC, Age, TUF etc are important.

# Do the ranges of the predictor variables make sense?

Yes,Approximately the predictor variable values are nearer to the actual values

# What are the distributions of the predictor variables?

They follow normal and binomial distributions.
"""

obesity_encoded_obesity.describe()

#Performing a Train-Test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(obesity_encoded_obesity, y, test_size=0.25, random_state=42)

"""Create a logistic regression model called lr. Include the parameter max_iter=1000 to make sure that the model will converge when you try to fit it.

Fit the model
Use the .fit() method on lr to fit the model to X and y.
"""

lr = LogisticRegression(max_iter=1000)

lr.fit(X_train,y_train)

"""# Model accuracy
A model's accuracy is the proportion of classes that the model correctly predicts. is Compute and print the accuracy of lr by using the .score() method. What percentage of respondents did the model correctly predict as being either obese or not obese? You may want to write this number down somewhere so that you can refer to it during future tasks.
"""

print(lr.score(X_train,y_train))

sfs = SFS(lr,
         k_features=10,
         forward = True,
         floating=False,
         cv=0,
         scoring='accuracy')

sfs.fit(X_train,y_train)

print(sfs.subsets_[10])

# Print the chosen feature names
print(print(sfs.subsets_[10]['feature_names']))
# Print the accuracy of the model after sequential forward selection
print(sfs.subsets_[10]['avg_score'])

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

plt.figure(figsize=(20, 30))  # Define the figure size before plotting
plot_sfs(sfs.get_metric_dict())  # Your plotting function should come after setting the figure size
plt.show()

from sklearn.model_selection import GridSearchCV, cross_val_score

# Define the parameter grid for Grid Search
param_grid = {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2'], 'max_iter': [100, 200, 500, 1000]}

# Perform Grid Search with cross-validation
grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Calculate mean and standard deviation of cross-validation scores
mean_cv_score = cross_val_scores.mean()
std_cv_score = cross_val_scores.std()

# Best parameters
best_params = grid_search.best_params_

# Initialize the logistic regression model with the best parameters
best_lr = LogisticRegression(**best_params)

# Cross-validation scores
cross_val_scores = cross_val_score(best_lr, X_train, y_train, cv=5, scoring='accuracy')

# Print the best parameters and cross-validation scores
print("Best Parameters:", best_params)
print("Cross-Validation Accuracy Scores:", cross_val_scores)

# Print mean and standard deviation
print(f"Mean Cross-Validation Accuracy: {mean_cv_score}")
print(f"Standard Deviation of Cross-Validation Accuracy: {std_cv_score}")

# Retrain the model on the entire dataset
best_lr.fit(obesity_encoded_obesity, y)

# If you have a separate test set, evaluate the model on it
# test_accuracy = best_lr.score(X_test, y_test) # Uncomment if you have a test set

# Final model evaluation
final_accuracy = best_lr.score(X_test, y_test)

# Print the final model accuracy
print(f"Final Model Accuracy on Training Data: {final_accuracy}")

"""# Imputation methods and checking the model"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.metrics import mean_squared_error


# Choose only numeric columns for simplicity in calculating errors
data_numeric = df.select_dtypes(include=[np.number])

# Split the data into training and test sets
X_train, X_test = train_test_split(data_numeric, test_size=0.2, random_state=42)

# Function to remove a random percentage of data
def remove_random_data(df, percentage):
    df_copy = df.copy()
    for column in df_copy.columns:
        num_missing = int(df_copy[column].shape[0] * percentage)
        missing_indices = np.random.choice(df_copy[column].shape[0], num_missing, replace=False)
        df_copy.iloc[missing_indices, df_copy.columns.get_loc(column)] = np.nan
    return df_copy

# Function to evaluate imputation
def evaluate_imputation(original_data, imputed_data):
    mse = mean_squared_error(original_data, imputed_data)
    return np.sqrt(mse)  # Return RMSE for easier interpretation

# Imputation methods
imputers = {
    'mean': SimpleImputer(strategy='mean'),
    'median': SimpleImputer(strategy='median'),
    'knn': KNNImputer(n_neighbors=5)
}
# Percentages of data to remove
percentages = [0.01, 0.05, 0.10]

# Store results
results = {}

for percentage in percentages:
    X_train_missing = remove_random_data(X_train, percentage)
    results[percentage] = {}

    for name, imputer in imputers.items():
        imputed = imputer.fit_transform(X_train_missing)
        rmse = evaluate_imputation(X_train, imputed)
        results[percentage][name] = rmse

# Display the results
for percentage, methods in results.items():
    print(f"\nPercentage of data removed: {percentage * 100}%")
    for method, rmse in methods.items():
        print(f"Imputation Method: {method}, RMSE: {rmse}")

# confusion matrix
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix


data = pd.read_csv('/content/ObesityDataSet_raw_and_data_sinthetic.csv')

target_variable = 'NObeyesdad'
predictor_variables = ['Gender',
                      'family_history_with_overweight',
                      'FAVC',
                      'CAEC',
                      'SMOKE',
                      'SCC',
                      'CALC',
                      'MTRANS']

data = pd.get_dummies(data, columns=predictor_variables)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(obesity_encoded_obesity, y, test_size=0.25, random_state=42)
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(max_depth=5, random_state=42)

# Train the model on your training data
model.fit(X_train, y_train)

# Obtain predictions on your testing data (or a separate dataset)
predicted_labels = model.predict(X_test)
predicted_labels = model.predict(X_test)
cm = confusion_matrix(y_test, predicted_labels)
print(cm)

# Create a visual representation of the confusion matrix
plt.imshow(cm, cmap=plt.cm.Blues)
plt.title("Confusion Matrix for NObeyesdad")
plt.colorbar()
plt.xticks(np.arange(len(np.unique(y_test))), np.unique(y_test), rotation=45)
plt.yticks(np.arange(len(np.unique(y_test))), np.unique(y_test))
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""Confusion Matrix for Categorical Columns"""

# Removing outliers
import numpy as np

def remove_outliers(data, method='z-score', threshold=3):
    """
    Removes outliers from a dataset using specified method and threshold.

    Args:
        data (list or array): The dataset to remove outliers from.
        method (str, optional): The outlier detection method. Defaults to 'z-score'.
        threshold (float, optional): The threshold for outlier identification. Defaults to 3.

    Returns:
        list or array: The dataset with outliers removed.
    """

    if method == 'z-score':
        # Calculate z-scores
        z_scores = np.abs((data - np.mean(data)) / np.std(data))

        # Remove data points with z-scores beyond the threshold
        filtered_data = data[z_scores <= threshold]

    elif method == 'iqr':
        # Calculate IQR
        Q1 = np.percentile(data, 25)
        Q3 = np.percentile(data, 75)
        IQR = Q3 - Q1

        # Remove data points outside of Q1 - 1.5*IQR and Q3 + 1.5*IQR
        filtered_data = data[(data >= Q1 - 1.5*IQR) & (data <= Q3 + 1.5*IQR)]

    else:
        raise ValueError("Invalid outlier detection method. Valid methods are 'z-score' and 'iqr'.")

    return filtered_data

